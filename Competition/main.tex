\documentclass[11pt]{article}

\usepackage{alltt,fullpage,graphics,color,epsfig,amsmath, amssymb}
\usepackage{hyperref}
\usepackage{boxedminipage}
\usepackage[ruled,vlined]{algorithm2e}

\newcommand{\floor}[1]{\lfloor #1 \rfloor}
\newcommand{\ceil}[1]{\lceil #1 \rceil}

\title{CS 510 Experimental Lab}
\author{Daniel Campos}
\date{April 14th,2021}
\begin{document}
\maketitle
\end{document}
\section{General Info}
Our team is blender, our team is: Daniel Campos and our task is the 2021 CLEF eHealth IR Task on consumer health search: Subtask 1
Team Name & Team Members}
\section{Goal of Participation}
The goal of participation in this task is to build real world experience on how to create a high quality search stack on a novel corpus. 
Besides understanding the steps required to build a search system I wish to understand how differences in retrieval mechanism effect relevance. In order to explore relevance I focused on the effect of major and minor tweaks to dense and sparse retrieval mechanisms. For explorations in sparse retreival I will be using BM25 and varying the parameters for K and B and RM3 parameters like term to expand, documents to expand, and original query term importance. For explorations in dense retrieval I will explore how Neural Language Models (BERT base) compare to sparse methods and how the two can be hybridized.
\section{Activities}
Our research activities were grouped around 3 main tasks: index creation, search system creation, and hyper parameter exploration. 

4) Sec 2. Activities:  Give a detailed description of the activities that your team have done. For example, we first tried XX and found it didn't work. We then tried YY, and found that ..... Or, we compared two methods, A and B. We optimized method A by ...., and optimized B by ..... Or, we focused on studying the effectiveness of method X and varies multiple parameters of the method including ....  (Write at least 0.5 page for this part.) 
\section{Findings}
The collection is about 480GB uncompressed and is distributed as a tar.gz compressed file; the size compressed is about 96 GB. Due to its size, we highly recommend the use of download manager software that enables resuming the download in case of failure. On Linux machines, this can be accomplished with
1,653 website domains.


In total, 2,021 domains were requested from the CommonCrawl dump of 2018-09. We successfully acquired data for 1,903. For the remaining domains, the CommonCrawl API returned an error or corrupted data (10 retries attempted), or incompatible data. Of the 1,903 crawled domains, 84 were not available in the CommonCrawl dump and for these, a folder in the corpus exists and represents the domain that was requested; however, the folder is empty (to mean that it was not available in the dump). Note that PDF documents were excluded from the data acquired from CommonCrawl. A complete list of domains and size of the crawl data for each domain is available

The topics are similar to 2018 CHS task topics: 50 queries, which were issued by the general public to the HON (Health on the Net) search service. These queries were manually selected by a domain expert from a sample of raw queries collected over a period of 6 months to be representative of the type of queries posed to the search engine. Queries were not preprocessed, for example any spelling mistakes that may be present have not been removed. Queries are numbered using a 6 digits number with the following convention: the first 3 digits of a query ID identified a topic number (information need) ranged from 151 to 200. The last 3 digits of a query ID identified each individual query creator
5) Sec. 3 Findings: Give a description of the findings that you have made via your experiments. This is similar to the part of a research paper on analysis of experiment results. Make sure to include at least one table or plot that shows the performance numbers of different methods or the same method with different parameter settings. Explain what the results tell us in terms of answering your research question. Do the results make sense to you? Try to provide an explanation of the results if you can. For example, why do you think method A performs better than method B? Or why do you think method A works better when the parameter is set to a smaller value? Also, try to look at the performance in detail by examining the performance on each single test instance. For example, in a retrieval experiment, the performance is often reported as an average over a set of different queries. Looking into the performance on each individual query can often reveal interesting findings that we can't easily see by only looking at the average performance.  (Write at least 0.5 page)
\section{Error Analysis}
To explore the failure of our model we looked at a query where our system has the lowest performance:{} 
In this query we find that 
6) Sec. 4. Error analysis: Try to look at at least one specific instance where a method has failed to see if you can find out any particular reason for the failure. Error analysis is a great way to gain interesting new insights about how to improve an existing method. (Try to write at least a few sentences about this, but no more than 0.5 page.) 
\section{Summary}
In this experimental lab what we have learned how a performant search engine can be setup on a novel search task. As shown in our results, we found that varing 
7) Sec. 5. Summery:  Briefly describe what you've learned from the Experimental Lab. (A few sentences would be sufficient.) 
\section{Task division}
Since Daniel is the only member of this group he has done everything. 