\documentclass[sigplan,screen]{acmart}
\AtBeginDocument{%`
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

\begin{document}
\title{CS510 Project Report: Compression Approaches for Neural Ranking and Question Answering}
\author{Daniel Campos}
\email{dcampos3@illinois.edu}
\begin{abstract}
Neural Networks have proven to be effective methods for retrieving relevant document however these methods can prove difficult to use in production. Neural models tend to be large and require specialized hardware for inference(GPU, TPU, FPGA, etc) which makes them difficult to deploy for all use cases. Researchers in fields like computer vision have leveraged methods in model compression such as structured and unstructured pruning to decrease model size while preserving or exceeding original model performance. In our work we will explore the interplay between unstructured pruning, model distillation, and layer removal in relation to question answering and passage re ranking. In our experiments we find that     
\end{abstract}
\keywords{information retrieval, network pruning, question answering}
\maketitle

\input{project/intro}
\input{project/related}
\input{project/method}
\input{project/experiment}
\input{project/conclude}
\bibliographystyle{ACM-Reference-Format}
\bibliography{bibliography}
\end{document}
\endinput

