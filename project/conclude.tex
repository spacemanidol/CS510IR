\section{Conclusion}
In our experiment we have shown that model distillation, layer dropping, and unstructured pruning are all powerful methods which can be used to make models substantially smaller without major accuracy losses. When focused on question answering, we are able to produce models that have less than 4\% decrease in performance despite being $\frac{1}{8}$ of the original size. Unlike previous methods exploring pruning for question answering, we are able to extract better results by enforcing uniform sparsity and leveraging GMP and long training regimes. Moreover, we showed that when knowledge distillation is employed, moderate pruning (40\%-70\%) can serve as a regularizer which helps the network out perform its original self. When focused on information Passage Ranking, the answer is less definitive. 
\section{Future Work}
In the future we wish to continue this work on model compression in information retrieval but will focus on Dense Passage Retrieval (DPR) \cite{Karpukhin2020DensePR} based methods. We believe these methods are of interest because compression of Siamese (Query encoder and document encoder) have not deeply studied. Given the success we saw in model size and even performance with pruning and distillation we believe we will likely find similar patterns with dual networks and we can use that to speed up index generation time. Currently, with a 20 million passage corpus DPR methods take 9 hours on a machine with 8 V100 GPUs for encoding and another 9 for indexing. With compression methods we believe we can likely cut that time in half. This is still orders of magnitude off of the Lucene sparse index generation which takes approximately 2 minutes. 
\section{Challenges}
This research project generally had two problems, performance replication and compute constraints. Replication performance lies in our inability to reproduce results cited by others in previous work and only plagued our experiments on the MSMARCO Passage Ranking corpus. Despite hours of tweaking the dataset, ranking methods, and broad hyperparameter sweeps, our best non-compressed experiment is far worse than methods in the literature. This was difficult because we spent a large portion of time trying to explore where failures may be and how to alleviate them instead of exploring a broader space of system compression. Our second challenge, compute, is a perpetual problem and has few solutions. Compression methodologies currently require many experiments with extended train times on large GPU clusters. Especially in the MSMARCO dataset where evaluation on the full validation set takes over 6 hours research cycles are slow. In order to deal with this problem we kept re adjusting project scope based on computational availability.  