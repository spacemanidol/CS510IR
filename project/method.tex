\section{Method}
To evaluate the effect of method compression we will explore how the combination of said methods effects two Information Retrieval tasks: Question Answering and Passage Retrieval. For each of the aforementioned tasks, we train tasks specific models built on bert-base-uncased. Given this baseline, we proceed to train variants with layers removed, with distillation, and with various degrees of unstructured pruning.
\subsection{Question Answering}
Question answering is the goal of providing an answer to a query $q$ given some sort of context passage $p$. While there are abstractive methods because of difficulty in evaluation we will focus on extractive methods. Extractive systems predict two attributes for each query and context passage: $token_{start}$ and $token_{end}$ each of which denotes an token index in a context passage. As a result, the answer $a$ to a query $q$ is $a=\{t_{token_{start}},...,t_{token_{end}}\}$. \\
The \textbf{S}tanford \textbf{QU}estion \textbf{A}nswering \textbf{D}ataset 1.1 (SQUAD) dataset has become the standard for extractive question answering. Constructed out of 500 context passages derived from Wikipedia it contains 100,000+ question-answer where each question was written by a human annotator. While there have been updates to the dataset and additional question answering datasets the widespread use of SQUAD makes it a logical choice. \\
\subsection{Passage Re-Ranking}
Passage ranking is a task which seeks to provide relevant short context passages for a given user query. These systems are commonly used as first stage retrievers for con question answering models. Since passages tend to have short context windows (under 512 tokens) transformer \cite{Vaswani2017AttentionIA} have become efficient models. With the introduction of large scale ranking datasets like MSMARCO \cite{Campos2016MSMA} and ODQA \cite{Karpukhin2020DensePR} data hungry neural methods thrived. Passage Retrieval has three main formulations, sparse retrieval,  dense retrieval, and hybrid retrieval. Sparse retrieval leverages traditional retrieval mechanisms like BM25 to rank relevant candidates. Dense retrieval leverages encoders which map queries and passages to a shared vector space and ranks candidates using an Approximate Nearest Neighbor index. Hybrid retrieval focuses on some combination of dense retrieval and commonly includes a sparse 1st stage ranker followed by a dense re-ranker. Our work focuses on Hybrid Retrieval on the MSMARCO Passage Retrieval dataset.\\
The MSMARCO dataset was originally a 1,000,000+ sample question answering dataset where given 10 context passages $p$, a human judge selected the most relevant passage and wrote the answer. This dataset was re-purposed for passage ranking by creating a collection of all unique passages and evaluating candidates ranking ability. Since the corpus does not contain a definitive ranking to optimize models train using negative and positive examples of query-passage relevance. Since relevance labels are binary, we follow standard training procedures and treat the task as a binary classification task. Given a query $q$ and a passage $p$ a model predicts how likely this pair is to belong in the relevant class. 
\subsection{Distillation}
Distillation can be considered an effective method for label smoothing \cite{Hinton2015DistillingTK}. Our focus is on self distillation where a teacher model is used to distill its learning into a student model. The teacher model is trained to convergence using cross entropy loss while the student is trained with a combination of the cross entropy loss and the Kullback–Leibler divergence between the student and teacher logits. Cross entropy is represented in equation \ref{cross}, and the distillation loss is shown in equation \ref{distill} where $sl$ are student logits, $tl$ are teacher logits, $\lambda$ is a distillation hardness,and $KL$ is the Kullback–Leibler divergence. 
\begin{equation}
L_{crossentropy} = - \sum_{i} ({y_i' \log(y_i) + (1-y_i') \log (1-y_i)})
\label{cross}
\end{equation}
\begin{equation}
L_{distill} = (1- \lambda )(L_{crossentropy}) + \lambda (KL(sl, tl))
\label{distill}
\end{equation}
\subsection{Compression}
Our work explores compression via unstructured pruning and layer removal. Language models like BERT are created by stacking identical layers of transformers and as a result compression can be achieved by simply removing these layers. We assign a mapping of layers to keep given target network depth based on experiments on most efficient methodologies and we find that for 1 layer networks we keep the first layer, for 3 we keep 1,6,12, for 6 we keep odd layers(1,3,5,7,9,11), for 9 we keep 1,3,4,5,7,8,9,10,12.
Unstructured pruning is the removal of weights in a network until a desired amount of weights remain. This is commonly achieved by setting weights in a network to zero or by adding a mask on top of network weights. Our work focuses on the the gradual removal of weight which have the smallest magnitude (magnitude pruning). Network layers are pruned independently of each other meaning that all components in a network share sparsity levels. Pruning is implement using Neural Magic's \footnote{https://neuralmagic.com/} sparseML library which introduces configurable sparsity in models with minimal overhead. \\
Layer removal is perhaps the simplest method we study as it solely focus on removing layers from a language model. Language models like BERT are created by stacking identical layers of transformers and as a result compression can be achieved by simply removing these layers. 