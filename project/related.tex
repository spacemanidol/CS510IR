\section{Related Work}
\subsection{Neural Rankers}
Like many computer science fields, Information Retrieval has seen neural network based systems out performing previous systems. Systems like DUET \cite{Mitra2017LearningTM}, DSSM \cite{Huang2013LearningDS}, C-KRNM \cite{10.1145/3159652.3159659}, and C-DSSM \cite{Shen2014LearningSR} leverage neural building blocks and classical IR ranking methodologies to produce results competitive with non neural methods. With the introduction of neural language models, countless BERT-based models produced new state of the art results similar to many other NLP fields \cite{nogueira2019multi}. Building on the success of the transformer and seeking to build document wide dependency modeling models leveraging the transformer such as Conformer-Kernel\cite{Mitra2020ConformerKernelWQ} have shown efficiency in quality and scale. In question answering, neural models have mirrored changes in ranking models. Approaches initially leveraged RNNs \cite{Shen2016ReasoNetLT,} \cite{Wang2017MachineCU} and quickly moved to include attention \cite{Seo2017BidirectionalAF} . With the introduction of BERT \cite{Devlin2019BERTPO}, improvement greatly improve and human performance was shortly outperformed.  
\subsection{Model Compression}
Neural network compression is an area that has attracted the attention of researchers for the last few decades. Methods for producing smaller networks that approximate original network performance include: distillation, quantization, structured and unstructured pruning. While each of these methods can compress models substantially on their own many researchers have found that some combination of these methods can produce the smallest models with the highest performance \cite{Polino2018ModelCV}, \cite{Sanh2020MovementPA}. \\
Model distillation \cite{Ba2014DoDN} addresses compressing models by first training a large network and calling it a teacher. Then using this teacher model a smaller student model learns to approximate what the teacher model would do. This framework is quite popular because it can leverage existing large models easily and the student model can be designed to fit the application requirements in terms of speed and model size. Distillation has been one of the most common methods of deployment of large scale language models where student models like DistilBERT \cite{Sanh2019DistilBERTAD} can approximate full model performance at a fraction of the size. \\
Model quantization \cite{Gong2014CompressingDC} \cite{Han2016DeepCC} addresses compressing models by reducing the number of bits that are require to represent parameters in a model. In simple implementations this means changing representation of weights from Float32 to float16(effectively cutting model size in half). Complex implementations tune networks to find the smallest amount of bits that can be used for weights, biases, and gradient updates using values as low as 1 bit \cite{Courbariaux2016BinarizedNN}. Quantization is particularly effective because it both leverages that networks are defaulted to a level of precision which is too high and by decreasing the size of representations networks are forced to share weights making networks more robust. \\
Model pruning \cite{LeCun1989OptimalBD} addresses model compression by decreasing the connection in a network. The goal of network pruning is to produce a sub network of the original network which optimizes some network property(accuracy, speed, robustness) while preserving the original network function. Network pruning has been show to produce a similar effect to random noise injection \cite{Bartoldson2019TheGT} and this noise can be used to make the network more efficient. Bartoldson et al., showed that network pruning is not just used for decreasing size but can be used to increase the generalization of the network. 
As mentioned earlier, there is structured pruning and unstructured pruning. In structured pruning, the structure of the network is altered by removal of entire neurons, layers, or portions neural network. This method has proven especially successful in language model compression where despite having dozens of attention heads \cite{Vaswani2017AttentionIA} few heads do most of the work and the rest can be removed \cite{Michel2019AreSH}. In unstructured pruning the network structure is altered by removal of individual weights. Unstructured pruning when paired with optimization engines can produce networks that are smaller, more accurate, and run faster than the original network. \\